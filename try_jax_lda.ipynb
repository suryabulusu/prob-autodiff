{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "try-jax-lda.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "id": "j8DQUm1KR7i2"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from pathlib import Path "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# limit the vocabulary V\n",
        "V = 1000\n",
        "# limit the topics \n",
        "T = 25\n",
        "# limit the epochs\n",
        "epochs = 50\n",
        "\n",
        "stop_words = [w.strip() for w in Path(\"stoplist.txt\").read_text().splitlines()]\n",
        "stop_words = set(stop_words) | set([\".\", \"-\", \"?\"])\n"
      ],
      "metadata": {
        "id": "FArXLAn_SpAv"
      },
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = Path(\"documents.txt\")\n",
        "\n",
        "def get_tokens(text):\n",
        "  t = text.strip().split(\"\\t\")[-1].lower().replace(\"?\", \" ?\").replace(\".\", \" .\").split()\n",
        "  return [w for w in t if w not in stop_words and len(w) > 4]\n",
        "  # someone should research how these tiny choices influence foundation models\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "c = Counter()\n",
        "for text in doc.read_text().splitlines():\n",
        "  toks = get_tokens(text)\n",
        "  for t in toks:\n",
        "    c[t] += 1\n"
      ],
      "metadata": {
        "id": "XmuAlnsoTSJu"
      },
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {v : idx for idx, (v, _) in enumerate(c.most_common(V))}\n",
        "# find the |V| most common words in counter c\n",
        "# and give each an idx\n",
        "# so the most commonly occuring word has idx 0\n",
        "# could even do the above with Counter(chain(*list_of_list_of_sentences))"
      ],
      "metadata": {
        "id": "Ji_LcXjGUIXA"
      },
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# time to encode data\n",
        "data = []\n",
        "labels = []\n",
        "for idx, text in enumerate(doc.read_text().splitlines()):\n",
        "  toks = get_tokens(text)\n",
        "  toks = [t for t in toks if t in vocab]\n",
        "  data += [vocab.get(t) for t in toks] # get index of t\n",
        "  labels += [idx] * len(toks) # the tokens belong to doc_idx\n",
        "# data -- list of words \n",
        "# labels -- the doc they belong to"
      ],
      "metadata": {
        "id": "U3NHlxVgVDcN"
      },
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N = len(data) # data size\n",
        "assert N == len(labels)\n",
        "\n",
        "print(N)"
      ],
      "metadata": {
        "id": "EeAsAf-AVOZ_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7080c62a-d7ad-46b5-85f5-b42254af060e"
      },
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "128056\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# M = len(Counter(labels))\n",
        "# the above is wrong.. because there could be docs in data not represented\n",
        "# ie, some doc_IDs do not occur in labels[] array\n",
        "M = idx + 1 # number of documents"
      ],
      "metadata": {
        "id": "4_hh4X9bOa0Z"
      },
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "key = jax.random.PRNGKey(0)\n",
        "\n",
        "data = jnp.array(data, dtype = jnp.int32)\n",
        "labels = jnp.array(labels, dtype = jnp.int32)"
      ],
      "metadata": {
        "id": "qOdisB73WPZZ"
      },
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_assigned = jax.random.randint(key, (N, ), 0, T, dtype = jnp.int32)\n",
        "# for each word in each doc -- assign a topic randomly from [0, T-1] range"
      ],
      "metadata": {
        "id": "qIrPN5LFWSCv"
      },
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_word = jax.ops.index_add(jnp.zeros((T, V), dtype = jnp.int32),\n",
        "                               jax.ops.index[topic_assigned, data], 1)\n",
        "# at indices of intersection -- add a 1\n",
        "# equivalent to for t,v in zip(topic_token, data): zeros_mat[t, v] = 1\n"
      ],
      "metadata": {
        "id": "c02OYLM1Xdm3"
      },
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_topic = jax.ops.index_add(jnp.zeros((M, T), dtype = jnp.int32),\n",
        "                               jax.ops.index[labels, topic_assigned], 1)\n",
        "# what are the topics present in each document\n",
        "# row -- each document; column -- topics"
      ],
      "metadata": {
        "id": "0ij8zS8GYK0q"
      },
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_word = jax.ops.index_add(jnp.zeros((M, V), dtype = jnp.int32),\n",
        "                              jax.ops.index[labels, data], 1)"
      ],
      "metadata": {
        "id": "FPtqvgdXOHtF"
      },
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jnp.sum(topic_word), jnp.sum(doc_topic), jnp.sum(doc_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmaMYalNPQTM",
        "outputId": "c208b5bb-6465-417a-8c6d-0680c485a285"
      },
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(DeviceArray(128056, dtype=int32),\n",
              " DeviceArray(128056, dtype=int32),\n",
              " DeviceArray(128056, dtype=int32))"
            ]
          },
          "metadata": {},
          "execution_count": 187
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(data), len(labels), M, N"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AD8_wi8EPZL8",
        "outputId": "66bf38bb-aac6-4559-d139-d34b6870416e"
      },
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(128056, 128056, 9743, 128056)"
            ]
          },
          "metadata": {},
          "execution_count": 188
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparams\n",
        "ALPHA, BETA = 0.1, 0.01"
      ],
      "metadata": {
        "id": "4Nxc4rJXT3fJ"
      },
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reverse_vocab = {v : k for k, v in vocab.items()}"
      ],
      "metadata": {
        "id": "EAoSMX5jT6lV"
      },
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def token_loop(state, scanned):\n",
        "  \"\"\"\n",
        "  runs once for every word in every document\n",
        "\n",
        "  (topic_word, doc_topic, topic_cnt) in state\n",
        "  topic_word [T x V]\n",
        "  doc_topic [M x T]\n",
        "  topic_cnt [T]\n",
        "\n",
        "  (topic_assigned, data, doc, key) in scanned\n",
        "  topic_assigned [1] -- for the current word, whats the topic assigned\n",
        "  data [1] -- a data point, ie, a word/token\n",
        "  doc [1] -- label of that data point -- ie, which document it belongs to\n",
        "  key [1] -- jax stuff\n",
        "  \"\"\"\n",
        "  topic_word, doc_topic, topic_cnt = state \n",
        "  topic_assigned, data, doc, key = scanned # current point we are looking at\n",
        "\n",
        "  local_tw = topic_word[:, data].at[topic_assigned].add(-1) # remove topic at which it belongs [T, V]\n",
        "  local_dt = doc_topic[doc].at[topic_assigned].add(-1) # remove corresponding token in doc [M, T]\n",
        "  local_tc = topic_cnt.at[topic_assigned].add(-1) # reduce cnt for that topic too [T]\n",
        "\n",
        "  # update the distribution \n",
        "  dist = ((local_tw + BETA) / (local_tc + V * BETA)) \\\n",
        "        * ((local_dt + ALPHA) / (doc_word.sum(axis = -1)[doc] + T * ALPHA)) # E [phi_tv] * E[theta_dt]\n",
        "  new_topic = jax.random.categorical(key, jnp.log(dist)) # sample topic from new dist\n",
        "\n",
        "  def update(_):\n",
        "    # state update after each scan\n",
        "    return (topic_word.at[new_topic, data].add(1).at[topic_assigned, data].add(-1),\n",
        "            doc_topic.at[doc, new_topic].add(1).at[doc, topic_assigned].add(-1),\n",
        "            topic_cnt.at[new_topic].add(1).at[topic_assigned].add(-1),\n",
        "    )\n",
        "    \n",
        "  return jax.lax.cond((new_topic != topic_assigned),\n",
        "                      update,\n",
        "                      lambda _ : (topic_word, doc_topic, topic_cnt), None\n",
        "         ), (new_topic, None, None, None)\n",
        "\n"
      ],
      "metadata": {
        "id": "5YbR7I04exrl"
      },
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mcmc(state):\n",
        "  \"\"\"\n",
        "  looks more like gibbs sampling -- since all samples are accepted\n",
        "  \n",
        "  topic_cnt [T x 1] -- number of words assigned to topic\n",
        "  topic_word [T x V] -- words in topic \n",
        "  doc_topic [M x T] -- topics in document\n",
        "  topic_assigned [N] -- topic assigned to each word; where N is total num of words\n",
        "  key -- jax stuff\n",
        "  \"\"\"\n",
        "  topic_cnt, topic_word, doc_topic, topic_assigned, key = state\n",
        "  keys = jax.random.split(key, N + 1) # split for each word .. and another for what/\n",
        "\n",
        "  (topic_word, doc_topic, topic_cnt), (topic_assigned, _ , _, _) = \\\n",
        "    jax.lax.scan(token_loop, # function \n",
        "                 (topic_word, doc_topic, topic_cnt), # init state\n",
        "                 (topic_assigned, data, labels, keys[1:])) # xs\n",
        "    # scan effectively scans through each data point.. for x in xs\n",
        "    # and accumulates result in state [kinda like a reduce function]\n",
        "\n",
        "  return topic_cnt, topic_word, doc_topic, topic_assigned, keys[0] # don't get the keys thing.. ignoring for now"
      ],
      "metadata": {
        "id": "AQ6cGPpV1vtz"
      },
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run(topic_word, doc_topic, topic_assigned):\n",
        "  key = jax.random.PRNGKey(1)\n",
        "  topic_cnt = topic_word.sum(axis = -1)\n",
        "  for i in range(50):\n",
        "    (topic_cnt, topic_word, doc_topic, topic_assigned, key) = \\\n",
        "      mcmc((topic_cnt, topic_word, doc_topic, topic_assigned, key))\n",
        "    return topic_word, doc_topic, topic_assigned\n",
        "\n",
        "topic_word, doc_topic, topic_assigned = run(topic_word, doc_topic, topic_assigned)"
      ],
      "metadata": {
        "id": "DiHZeu87zsYX"
      },
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yv7DxaQqEd-n",
        "outputId": "3133d86b-6b95-47a7-bf7d-51b61a14670f"
      },
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([ 4,  2,  2,  2, 14,  7, 21, 21,  2,  2], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 195
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out = topic_word / topic_word.sum(axis = -1, keepdims = True)\n",
        "\n",
        "for i in range(T):\n",
        "  print(\"TOPIC\", i, [reverse_vocab[int(x)] for x in reversed(jnp.argsort(out[i])[-5:])])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FL7t24D5BwT",
        "outputId": "635945da-a3c9-4d4a-f9e3-1266f1a71080"
      },
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TOPIC 0 ['america', 'nation', 'health', 'national', 'american']\n",
            "TOPIC 1 ['government', 'states', 'nation', 'america', 'federal']\n",
            "TOPIC 2 ['government', 'country', 'security', 'united', 'freedom']\n",
            "TOPIC 3 ['government', 'american', 'program', 'america', 'against']\n",
            "TOPIC 4 ['under', 'states', 'economic', 'american', 'public']\n",
            "TOPIC 5 ['government', 'about', 'american', 'defense', 'military']\n",
            "TOPIC 6 ['american', 'federal', 'billion', 'country', 'government']\n",
            "TOPIC 7 ['american', 'economic', 'federal', 'states', 'present']\n",
            "TOPIC 8 ['government', 'national', 'peace', 'nation', 'federal']\n",
            "TOPIC 9 ['government', 'national', 'public', 'nations', 'american']\n",
            "TOPIC 10 ['states', 'national', 'federal', 'without', 'about']\n",
            "TOPIC 11 ['american', 'country', 'strengthen', 'americans', 'security']\n",
            "TOPIC 12 ['government', 'national', 'nation', 'american', 'right']\n",
            "TOPIC 13 ['american', 'national', 'government', 'united', 'program']\n",
            "TOPIC 14 ['government', 'national', 'american', 'america', 'united']\n",
            "TOPIC 15 ['government', 'federal', 'country', 'america', 'states']\n",
            "TOPIC 16 ['government', 'american', 'against', 'states', 'united']\n",
            "TOPIC 17 ['government', 'federal', 'states', 'billion', 'million']\n",
            "TOPIC 18 ['government', 'peace', 'american', 'america', 'against']\n",
            "TOPIC 19 ['government', 'federal', 'united', 'peace', 'states']\n",
            "TOPIC 20 ['government', 'america', 'federal', 'national', 'business']\n",
            "TOPIC 21 ['government', 'american', 'peace', 'problems', 'through']\n",
            "TOPIC 22 ['government', 'federal', 'peace', 'nation', 'necessary']\n",
            "TOPIC 23 ['government', 'united', 'nations', 'american', 'through']\n",
            "TOPIC 24 ['national', 'government', 'america', 'public', 'security']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# how to use gradients... to improve this\n",
        "# what if the sampling distrib was fancier\n",
        "# or sampling was HMC\n",
        "# will have to sample latents with requires_grad = True"
      ],
      "metadata": {
        "id": "-cprt-h6Ck2W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}